<sub>&#x1F6A8; <strong>Autogenerated!</strong> See <a href="https://github.com/ponyfoo/articles/tree/noindex/contributing.markdown"><code>contributing.markdown</code></a> for details. See also: <a href="https://ponyfoo.com/articles/an-elastic-stack-primer">web version</a>.</sub>

<a href="https://ponyfoo.com/articles/an-elastic-stack-primer"><div><img src="https://i.imgur.com/Ej7olDZ.jpg" alt="An Elastic Stack Primer"></div></a>

<h1>An Elastic Stack Primer</h1>

<p><kbd>elasticsearch</kbd> <kbd>logstash</kbd> <kbd>kibana</kbd></p>

<blockquote><p>This article describes my adventures while getting initiated into the Elastic Stack. We&#x2019;ll be building upon the <code>elasticsearch</code> index I&#x2019;ve set up for search in an earlier post.</p>
</blockquote>

<div><p>This article describes my adventures while getting initiated into the Elastic Stack. We&#x2019;ll be building upon the <code class="md-code md-code-inline">elasticsearch</code> index I&#x2019;ve set up for search in an earlier post. We&#x2019;ll upgrade our stack to <code class="md-code md-code-inline">5.x</code>, incorporate <code class="md-code md-code-inline">logstash</code> and <code class="md-code md-code-inline">kibana</code>, so we can <strong>consume and visualize data</strong> <em>&#x2013; besides what our search interface allows</em>; and we&#x2019;ll also hone and troubleshoot our understanding of the Elastic Stack along the way.</p></div>

<blockquote></blockquote>

<div><p>Getting started with the <a href="https://www.elastic.co/v5" target="_blank" rel="noopener noreferrer" aria-label="Say &#x201C;Heya&#x201D; to the Elastic Stack">Elastic Stack</a> can feel a bit overwhelming. You need to set up <code class="md-code md-code-inline">elasticsearch</code>, Kibana, and <code class="md-code md-code-inline">logstash</code> before you even can get to the fun parts &#x2013; maybe even before you fully understand how the three synergize providing you with formidable and formerly untapped insights into your platform.</p> <p>I have been running Pony Foo since late 2012, and, <em>although I had set up Google Analytics and Clicky since the very beginning</em>, these kinds of user tracking systems are far from ideal when it comes to troubleshooting. In the years since I launched the blog I tried a couple of instrumentation tools that would provide me with reporting and metrics from the Node.js application for my blog, but these solutions would require me to, well&#x2026; <em>instrument</em> the Node apps by patching them with a snippet of code that would then communicate with a third party service. Visiting that service I could learn more about the current state of my production application. For a variety of reasons, <em>&#x2013; too expensive, makes my app too slow, etc. &#x2013;</em> I ended up ditching every one of these solutions not long after giving them a shot.</p> <p>Not long ago I wrote about <a href="https://ponyfoo.com/articles/setting-up-elasticsearch-for-a-blog" aria-label="Setting Up Elasticsearch for a Blog on Pony Foo">setting up search for a blog</a>, and since I&#x2019;m already working on the Kibana analytics dashboard team I figured it wouldn&#x2019;t hurt to learn more about <code class="md-code md-code-inline">logstash</code> so that I could go full circle and finally have a look at some server metrics my way.</p> <p><strong>Logstash is, in short, a way to get data from one place to another.</strong> It helps stream events pulled out of files, HTTP requests, tweets, event logs, or <a href="https://www.elastic.co/guide/en/logstash/current/input-plugins.html" target="_blank" rel="noopener noreferrer" aria-label="Logstash input plugins">dozens of other input sources</a>. After processing events, Logstash can output them via Elasticsearch, disk files, email, HTTP requests, or <a href="https://www.elastic.co/guide/en/logstash/current/output-plugins.html" target="_blank" rel="noopener noreferrer" aria-label="Logstash output plugins">dozens of other output plugins</a>.</p> <figure class="figure-has-loaded"><a href="https://www.elastic.co/guide/en/logstash/current/introduction.html" target="_blank" rel="noopener noreferrer" aria-label="Introduction to Logstash"><img src="https://i.imgur.com/2nevyPE.png" alt="Logstash inputs and outputs" title="logstash.png"></a></figure> <p>The above graph shows how Logstash can provide tremendous value through a relatively simple interface where we define inputs and outputs. You could easily use it to pull a Twitter firehose of political keywords about a presidential campaign into Elasticsearch for further analysis. Or to anticipate breaking news on Twitter as they occur.</p> <p>Or maybe you have more worldly concerns, like streaming log events from <code class="md-code md-code-inline">nginx</code> and <code class="md-code md-code-inline">node</code> into <code class="md-code md-code-inline">elasticsearch</code> for increased visibility through a Kibana dashboard. That&#x2019;s what we&#x2019;re going to do in this article.</p></div>

<div><h1 id="installing-the-elastic-stack">Installing the Elastic Stack</h1> <p>I had used <em>Elasticsearch 2.3</em> for <a href="https://ponyfoo.com/articles/setting-up-elasticsearch-for-a-blog" aria-label="Setting Up Elasticsearch for a Blog on Pony Foo">my previous blog post</a>. This represented a problem when it came to using the Elastic Stack, because I wanted to use the latest version of Kibana <em>&#x2013; <code class="md-code md-code-inline">5.0.0-alpha3</code>, at the time of this writing.</em></p> <p>As mentioned in the earlier article, we&#x2019;ll have to download and install Java 8 to get Elasticsearch up and running properly. Below is the piece of code we used to install Java 8. Keep in mind this code was tested on a Debian Jessie environment, but it should mostly work in Ubuntu or similar systems.</p> <pre class="md-code-block"><code class="md-code md-lang-bash"><span class="md-code-comment"># install java 8</span>
<span class="md-code-built_in">export</span> JAVA_DIST=<span class="md-code-number">8</span>u92-b14
<span class="md-code-built_in">export</span> JAVA_RESOURCE=jdk-<span class="md-code-number">8</span>u92-linux-x64.tar.gz
<span class="md-code-built_in">export</span> JAVA_VERSION=jdk1.<span class="md-code-number">8.0</span>_92
wget -nv --header <span class="md-code-string">&quot;Cookie: oraclelicense=accept-securebackup-cookie&quot;</span> -O /tmp/java-jdk.tar.gz http://download.oracle.com/otn-pub/java/jdk/<span class="md-code-variable">$JAVA_DIST</span>/<span class="md-code-variable">$JAVA_RESOURCE</span>
sudo mkdir /opt/jdk
sudo tar -zxf /tmp/java-jdk.tar.gz -C /opt/jdk
sudo update-alternatives --install /usr/bin/java java /opt/jdk/<span class="md-code-variable">$JAVA_VERSION</span>/bin/java <span class="md-code-number">100</span>
sudo update-alternatives --install /usr/bin/javac javac /opt/jdk/<span class="md-code-variable">$JAVA_VERSION</span>/bin/javac <span class="md-code-number">100</span>
</code></pre> <p>Next, we&#x2019;ll install the entire pre-release Elastic Stack <em>&#x2013; formerly ELK Stack</em>, for <strong>E</strong>lasticsearch, <strong>L</strong>ogstash and <strong>K</strong>ibana. The following piece of code pulls all three packages from <code class="md-code md-code-inline">packages.elastic.co</code> and installs them.</p> <pre class="md-code-block"><code class="md-code md-lang-bash"><span class="md-code-comment"># install elasticsearch + logstash + kibana</span>
wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
<span class="md-code-built_in">echo</span> <span class="md-code-string">&quot;deb https://packages.elastic.co/elasticsearch/5.x/debian stable main&quot;</span> | sudo tee <span class="md-code-operator">-a</span> /etc/apt/sources.list
<span class="md-code-built_in">echo</span> <span class="md-code-string">&quot;deb https://packages.elastic.co/logstash/5.0/debian stable main&quot;</span> | sudo tee <span class="md-code-operator">-a</span> /etc/apt/sources.list
<span class="md-code-built_in">echo</span> <span class="md-code-string">&quot;deb https://packages.elastic.co/kibana/5.0.0-alpha/debian stable main&quot;</span> | sudo tee <span class="md-code-operator">-a</span> /etc/apt/sources.list
sudo apt-get update
sudo apt-get -y install elasticsearch logstash kibana
</code></pre> <p>After installing all three, we should turn on their services so that they run at startup. This is particularly desirable in production systems. Debian Jessie relies on <code class="md-code md-code-inline">systemd</code> for services, so that&#x2019;s what we&#x2019;ll use to enable these services.</p> <pre class="md-code-block"><code class="md-code md-lang-bash"><span class="md-code-comment"># elasticsearch + logstash + kibana as services</span>
sudo /bin/systemctl daemon-reload
sudo /bin/systemctl <span class="md-code-built_in">enable</span> elasticsearch.service
sudo /bin/systemctl <span class="md-code-built_in">enable</span> logstash.service
sudo /bin/systemctl <span class="md-code-built_in">enable</span> kibana.service
</code></pre> <blockquote> <p>You want to reduce your &#x201C;hands on&#x201D; &#x1F44F; production experience as much as possible. Automation is king, etc. Otherwise, why go through the trouble?</p> </blockquote> <p>Lastly, we start all of the services and log their current status. We pause for ten seconds after booting Elasticsearch in order to give it time to start listening for connections, so that Logstash doesn&#x2019;t run into any trouble.</p> <pre class="md-code-block"><code class="md-code md-lang-bash"><span class="md-code-comment"># firing up elasticsearch</span>
sudo service elasticsearch restart || sudo service elasticsearch start
sudo service elasticsearch status

<span class="md-code-comment"># giving logstash some runway</span>
sleep <span class="md-code-number">10</span>s

<span class="md-code-comment"># firing up logstash</span>
sudo service logstash restart || sudo service logstash start
sudo service logstash status

<span class="md-code-comment"># firing up kibana</span>
sudo service kibana restart || sudo service kibana start
sudo service kibana status
</code></pre> <p>Great, now we&#x2019;ll need to tweak our <code class="md-code md-code-inline">nginx</code> server.</p> <h1 id="setting-up-nginx">Setting up <code class="md-code md-code-inline">nginx</code></h1> <p>First, I exposed Kibana on my <code class="md-code md-code-inline">nginx</code> server configuration. I chose to password protect my Kibana instance using basic authentication. To do that, you need an <code class="md-code md-code-inline">htpasswd</code> file like so, where <code class="md-code md-code-inline">$USERNAME</code> would contain the basic authentication username and <code class="md-code md-code-inline">$PASSWORD</code> would be the password for that user.</p> <pre class="md-code-block"><code class="md-code md-lang-bash">htpasswd -nb <span class="md-code-variable">$USERNAME</span> <span class="md-code-variable">$PASSWORD</span> &gt; <span class="md-code-variable">$FILENAME</span>
</code></pre> <p>That&#x2019;ll create a file such as the following, where we have <code class="md-code md-code-inline">user/pass</code> for authentication and <code class="md-code md-code-inline">pass</code> was encrypted.</p> <pre class="md-code-block"><code class="md-code">user:$apr1$MZLc648z$MtfQUP9Kz5216Wafq5j9/1
</code></pre> <p>When setting up the <code class="md-code md-code-inline">nginx</code> proxy server, you need to use the <code class="md-code md-code-inline">auth_basic</code> and <code class="md-code md-code-inline">auth_basic_user_file</code> directives, as shown below. Make sure to replace <code class="md-code md-code-inline">$FILENAME</code> with the contents of the <code class="md-code md-code-inline">$FILENAME</code> variable used above.</p> <pre class="md-code-block"><code class="md-code">server {
  auth_basic &quot;kibana_restricted&quot;;
  auth_basic_user_file <mark class="md-mark md-code-mark">$FILENAME</mark>;
}
</code></pre> <blockquote> <h3 id="notes-about-x-pack">Notes About X-Pack</h3> <p><em>A more robust alternative to proxying via nginx in order to password-protect Kibana would be using <a href="https://www.elastic.co/v5" target="_blank" rel="noopener noreferrer" aria-label="Say Heya to the Elastic Stack">X-Pack and Shield</a>. X-Pack is a commercial offering that introduces an extra protection layer in front of Elasticsearch. That same authentication layer can then be leveraged to password-protect Kibana.</em><br> <em>Other X-Pack features include cluster health monitoring, data change alerts, reporting, and relationship graphs.</em></p> <p>I&#x2019;ve set it up myself for my blog, and I must say that X-Pack is pretty awesome. Check out <a href="https://github.com/ponyfoo/ponyfoo/blob/2b4bd84cad9aa20e2a326d715259d1c9b499b68e/deploy/templates/primal#L13-L41" target="_blank" rel="noopener noreferrer" aria-label="Deploying X-Pack for Pony Foo">my deployment scripts</a> to learn more about setting it up. In this article, I&#x2019;ve decided against providing a more detailed description of the installation process and features in X-Pack. If you want to give it a shot, we offer <a href="https://www.elastic.co/downloads/x-pack" target="_blank" rel="noopener noreferrer" aria-label="Download X-Pack for the Elastic Stack">a month-long trial</a> where you can take all of these extra awesome features for a ride.</p> </blockquote> <p>It&#x2019;s important to note that we&#x2019;ll be expecting the <code class="md-code md-code-inline">access.log</code> file to have a specific format that Logstash understands, so that we can consume it and split it into discrete fields. To configure <code class="md-code md-code-inline">nginx</code> with the pattern we&#x2019;ll call metrics, add this to the <code class="md-code md-code-inline">http</code> section of your <code class="md-code md-code-inline">nginx.conf</code> file.</p> <pre class="md-code-block"><code class="md-code">log_format metrics &apos;$http_host &apos;
                   &apos;$proxy_protocol_addr [$time_local] &apos;
                   &apos;&quot;$request&quot; $status $body_bytes_sent &apos;
                   &apos;&quot;$http_referer&quot; &quot;$http_user_agent&quot; &apos;
                   &apos;$request_time &apos;
                   &apos;$upstream_response_time&apos;;
</code></pre> <p>Next, in the <code class="md-code md-code-inline">server</code> section of your <code class="md-code md-code-inline">site.conf</code> for the site you want to be parsing logs for, just tell <code class="md-code md-code-inline">nginx</code> to use the <code class="md-code md-code-inline">metrics</code> logging format we&#x2019;ve defined earlier.</p> <pre class="md-code-block"><code class="md-code">access_log /var/log/nginx/ponyfoo.com.access.log metrics;
</code></pre> <p>After exposing the Kibana web app on my <code class="md-code md-code-inline">nginx</code> server and pointing a DNS <code class="md-code md-code-inline">CNAME</code> entry for <a href="https://analytics.ponyfoo.com/" target="_blank" rel="noopener noreferrer" aria-label="Kibana for Pony Foo">analytics.ponyfoo.com</a> <em>(currently password protected)</em> to my load balancer, I was good to go.</p> <h1 id="from-nginx-to-elasticsearch-via-logstash">&#x1F6BF; From <code class="md-code md-code-inline">nginx</code> to Elasticsearch via Logstash</h1> <p>Logstash relies on inputs, filters applied on those inputs, and outputs. We configure all of this in a <code class="md-code md-code-inline">logstash.conf</code> config file that we can then feed logstash using the <code class="md-code md-code-inline">logstash -f $FILE</code> command.</p> <p>Getting Logstash to read <code class="md-code md-code-inline">nginx</code> access logs is easy. You specify an event type that&#x2019;s later going to be used when indexing events into Elasticsearch, and the path to the file where your logs are recorded. Logstash takes care of tailing the file and streaming those logs to whatever outputs you indicate.</p> <pre class="md-code-block"><code class="md-code">input {
  file {
    type =&gt; &quot;nginx_access&quot;
    path =&gt; &quot;/var/log/nginx/ponyfoo.com.access.log&quot;
  }
}
</code></pre> <p>When it comes to configuring Logstash to pipe its output into Elasticsearch, the configuration <em>couldn&#x2019;t be any simpler</em>.</p> <pre class="md-code-block"><code class="md-code">output {
  elasticsearch {}
}
</code></pre> <p>An important aspect of using Logstash is using filters to determine the fields and field types you&#x2019;ll use when indexing log events into Elasticsearch. The following snippet of code tells Logstash to look for <code class="md-code md-code-inline">grok</code> patterns in <code class="md-code md-code-inline">/opt/logstash/patterns</code>, and to match the <code class="md-code md-code-inline">message</code> field to the <code class="md-code md-code-inline">NGINX_ACCESS</code> grok pattern, which is defined in a patterns file that lives in the provided directory.</p> <pre class="md-code-block"><code class="md-code">filter {
  grok {
    patterns_dir =&gt; &quot;/opt/logstash/patterns&quot;
    match =&gt; { &quot;message&quot; =&gt; &quot;%{NGINX_ACCESS}&quot; }
  }
}
</code></pre> <p>The contents of the file containing the grok patterns are outlined below, and they should be placed in a file in <code class="md-code md-code-inline">/opt/logstash/patterns</code>, as specified in the <code class="md-code md-code-inline">patterns_dir</code> directive above. Logstash comes with <a href="https://github.com/logstash-plugins/logstash-patterns-core/tree/02fc1c47e094cbbe3e482754cda95088827da327/patterns" target="_blank" rel="noopener noreferrer" aria-label="Grok patterns shipped with Logstash">quite a few built-in patterns</a> and the general way of consuming a pattern is <code class="md-code md-code-inline">%{PATTERN:name}</code> where some patterns such as <code class="md-code md-code-inline">NUMBER</code> also take a type argument, such as <code class="md-code md-code-inline">%{NUMBER:bytes:int}</code>, in the piece of code below.</p> <pre class="md-code-block"><code class="md-code">NGINX_USERNAME [a-zA-Z\.\@\-\+_%]+
NGINX_USER %{NGINX_USERNAME}
NGINX_ACCESS %{IPORHOST:http_host} %{IPORHOST:client_ip} \[%{HTTPDATE:timestamp}\] \&quot;(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:http_version:float})?|%{DATA:raw_request})\&quot; %{NUMBER:status:int} (?:%{NUMBER:bytes:int}|-) %{QS:referrer} %{QS:agent} %{NUMBER:request_time:float} %{NUMBER:upstream_time:float}
NGINX_ACCESS %{IPORHOST:http_host} %{IPORHOST:client_ip} \[%{HTTPDATE:timestamp}\] \&quot;(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:http_version:float})?|%{DATA:raw_request})\&quot; %{NUMBER:status:int} (?:%{NUMBER:bytes:int}|-) %{QS:referrer} %{QS:agent} %{NUMBER:request_time:float}
</code></pre> <p>Each match in the pattern above means a field will be created for that bit of output, with its corresponding name and type as indicated in the pattern file.</p> <p>Going back to the <code class="md-code md-code-inline">filter</code> section of our <code class="md-code md-code-inline">logstash.conf</code> file, we&#x2019;ll also want a <a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-data.html" target="_blank" rel="noopener noreferrer" aria-label="Logstash Date Filter"><code class="md-code md-code-inline">date</code></a> filter that parses the <code class="md-code md-code-inline">timestamp</code> field into a proper date. a <a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-geoip.html" target="_blank" rel="noopener noreferrer" aria-label="Logstash Geoip Filter"><code class="md-code md-code-inline">geoip</code></a> filter so that the <code class="md-code md-code-inline">client_ip</code> field gets the geolocation treatment.</p> <pre class="md-code-block"><code class="md-code">filter {
  grok {
    patterns_dir =&gt; &quot;/opt/logstash/patterns&quot;
    match =&gt; { &quot;message&quot; =&gt; &quot;%{NGINX_ACCESS}&quot; }
  }
  date {
    match =&gt; [ &quot;timestamp&quot; , &quot;dd/MMM/YYYY:HH:mm:ss Z&quot; ]
  }
  geoip {
    source =&gt; &quot;client_ip&quot;
  }
}
</code></pre> <blockquote> <p>You can run <code class="md-code md-code-inline">logstash -f $FILE</code> to test the configuration file and ensure your pattern works. There&#x2019;s also a website where you can <a href="http://grokconstructor.appspot.com/do/match" target="_blank" rel="noopener noreferrer" aria-label="Test grok patterns online">test your <code class="md-code md-code-inline">grok</code> patterns online</a>.</p> </blockquote> <p>If you&#x2019;re running Logstash as a service, or intend to, you&#x2019;ll need to place the <code class="md-code md-code-inline">logstash.conf</code> file in <code class="md-code md-code-inline">/etc/logstash/conf.d</code> <em>&#x2013; or change the default configuration directory.</em></p> <p>My preference when configuring globally-installed tools such as Logstash is to keep configuration files in a centralized location and then create symbolic links in the places where the tool is looking for those files. In the case of Logstash there&#x2019;s two types of files we&#x2019;ll be using: <code class="md-code md-code-inline">grok</code> patterns and Logstash configuration files. I&#x2019;ll be keeping them in <code class="md-code md-code-inline">$HOME/app/logstash</code> and creating links to the places where Logstash looks for each of these kinds of files.</p> <pre class="md-code-block"><code class="md-code md-lang-bash">sudo ln -sfn <span class="md-code-variable">$HOME</span>/app/logstash/config/*.conf /etc/logstash/conf.d/
sudo ln -sfn <span class="md-code-variable">$HOME</span>/app/logstash/patterns/* /opt/logstash/patterns/
</code></pre> <p>By the way, I had an issue where <code class="md-code md-code-inline">nginx</code> logs wouldn&#x2019;t be piped into Elasticsearch, and Logstash would fail silently, <strong>but only when run as a service</strong>. It turned out to be a file permissions issue, one that was easily fixed by ensuring that the <code class="md-code md-code-inline">logstash</code> user had execute permissions on every directory from <code class="md-code md-code-inline">/</code> to <code class="md-code md-code-inline">/var/log/nginx</code>.</p> <p>You can verify that by executing the following command, which interpolates <a href="http://www.linuxcommand.org/man_pages/getfacl1.html" target="_blank" rel="noopener noreferrer" aria-label="The getfacl command explained"><code class="md-code md-code-inline">getfacl /</code></a> all the way through to the <code class="md-code md-code-inline">/var/log/nginx</code> directory.</p> <pre class="md-code-block"><code class="md-code md-lang-bash">getfacl /{,var/{,<span class="md-code-built_in">log</span>/{,nginx}}}
</code></pre> <p>You&#x2019;ll get output such as this:</p> <pre class="md-code-block"><code class="md-code">getfacl: Removing leading &apos;/&apos; from absolute path names
# file: var/log/nginx
# owner: www-data
# group: adm
user::rwx
group::r-x
other::---
</code></pre> <p>You&#x2019;ll note that in this case, user <code class="md-code md-code-inline">logstash</code> of group <code class="md-code md-code-inline">logstash</code> wouldn&#x2019;t have execute rights on <code class="md-code md-code-inline">/var/log/nginx</code>. This can be easily fixed with the following command, but it gave me quite the headache until I stumbled upon this particular oddity.</p> <pre class="md-code-block"><code class="md-code">sudo chmod o+x /var/log/nginx
</code></pre> <blockquote> <p><sub><em>Naturally, you&#x2019;ll also need to grant read access to the relevant log files.</em></sub></p> </blockquote> <p>I didn&#x2019;t just want to stream <code class="md-code md-code-inline">nginx</code> logs to Elasticsearch, I also wanted <code class="md-code md-code-inline">node</code> application level logs to be here. Let&#x2019;s do this!</p> <h1 id="throwing-node-logs-into-the-mix">&#x1F300; Throwing <code class="md-code md-code-inline">node</code> Logs into the Mix</h1> <p>To add Node logs into the mix, we don&#x2019;t need to change the <code class="md-code md-code-inline">output</code> section of our Logstash file. It has all we needed. We do need to change the <code class="md-code md-code-inline">input</code> and <code class="md-code md-code-inline">filter</code> sections. First, we&#x2019;ll add another <code class="md-code md-code-inline">file</code> to the <code class="md-code md-code-inline">input</code> section. This should point to the file where your Node.js app is streaming its logs into. In my case that file was <code class="md-code md-code-inline">/var/log/ponyfoo-production.log</code>, which is where the service running my <code class="md-code md-code-inline">node</code> app redirected its output.</p> <pre class="md-code-block"><code class="md-code">input {
  file {
    type =&gt; &quot;node_app&quot;
    path =&gt; &quot;/var/log/ponyfoo-production.log&quot;
  }
}
</code></pre> <p>Now, the <code class="md-code md-code-inline">node_app</code> log is a bit different. Sometimes, there are multi-line log entries. That happens when I log a stack trace. That&#x2019;s easily fixed by adding <a href="https://www.elastic.co/guide/en/logstash/current/plugins-codecs-multiline.html" target="_blank" rel="noopener noreferrer" aria-label="Multi-line Logstash Codec">a <code class="md-code md-code-inline">multiline</code> codec</a> to my input processor, where each log entry must start with a <code class="md-code md-code-inline">NODE_TIME</code> pattern-matching string, or otherwise is merged into the current log entry. Effectively, this means that lines that don&#x2019;t start with a timestamp will be treated as part of the last event, until a new line that matches a timestamp comes along, creating a new entry.</p> <pre class="md-code-block"><code class="md-code">input {
  file {
    type =&gt; &quot;node_app&quot;
    path =&gt; &quot;/var/log/ponyfoo-production.log&quot;
    <mark class="md-mark md-code-mark">codec =&gt; multiline</mark> {
      patterns_dir =&gt; &quot;/opt/logstash/patterns&quot;
      pattern =&gt; &quot;^%{NODE_TIME} &quot;
      what =&gt; &quot;previous&quot;
      negate =&gt; true
    }
  }
}
</code></pre> <p>My Node.js logs are quite compact, containing a date and time, a <code class="md-code md-code-inline">level</code> marker and the logged message.</p> <pre class="md-code-block"><code class="md-code">8 Jun 21:53:27 - info: Database connection to {db:ponyfoo-cluster} established.
08 Jun 21:53:27 - info: Worker 756 executing app@1.0.37
08 Jun 21:53:40 - info: elasticsearch: Adding connection to http://localhost:9200/
08 Jun 21:53:40 - info: cluster listening on port 3000.
08 Jun 21:56:07 - info: Database connection to {db:ponyfoo-cluster} established.
08 Jun 21:56:07 - info: Worker 913 executing app@1.0.37
08 Jun 21:56:07 - info: elasticsearch: Adding connection to http://localhost:9200/
08 Jun 21:56:08 - info: Database connection to {db:ponyfoo-cluster} established.
08 Jun 21:56:08 - info: Worker 914 executing app@1.0.37
08 Jun 21:56:08 - info: elasticsearch: Adding connection to http://localhost:9200/
08 Jun 21:56:08 - info: app listening on ip-10-0-69-59:3000
08 Jun 21:56:09 - info: app listening on ip-10-0-69-59:3000
</code></pre> <p>As a result, my Node.js patterns file is much simpler. The <code class="md-code md-code-inline">NODE_TIME</code> was exacted into its own pattern for convenience.</p> <pre class="md-code-block"><code class="md-code">NODE_TIME %{MONTHDAY} %{MONTH} %{TIME}
NODE_APP %{NODE_TIME:timestamp} - %{LOGLEVEL:level}: %{GREEDYDATA:description}
</code></pre> <p>In order for the <code class="md-code md-code-inline">filter</code> section to play nice with two different input files under different formats, we&#x2019;ll have to update it using conditionals. The following piece of code is the same <code class="md-code md-code-inline">filter</code> section we had earlier, wrapped in an <code class="md-code md-code-inline">if</code> that checks whether we are dealing with an event of type <code class="md-code md-code-inline">nginx_access</code>.</p> <pre class="md-code-block"><code class="md-code">filter {
  <mark class="md-mark md-code-mark">if [type] == &quot;nginx_access&quot; {</mark>
    grok {
      patterns_dir =&gt; &quot;/opt/logstash/patterns&quot;
      match =&gt; { &quot;message&quot; =&gt; &quot;%{NGINX_ACCESS}&quot; }
    }
    date {
      match =&gt; [ &quot;timestamp&quot; , &quot;dd/MMM/YYYY:HH:mm:ss Z&quot; ]
    }
    geoip {
      source =&gt; &quot;client_ip&quot;
    }
  <mark class="md-mark md-code-mark">}</mark>
}
</code></pre> <p>For <code class="md-code md-code-inline">node_app</code> events, we&#x2019;ll do something quite similar, except there&#x2019;s no geolocation going on, and our main <code class="md-code md-code-inline">message</code> pattern is named <code class="md-code md-code-inline">NODE_APP</code> instead of <code class="md-code md-code-inline">NGINX_ACCESS</code>. Also, make sure that the filter matches the new document type, <code class="md-code md-code-inline">node_type</code>.</p> <pre class="md-code-block"><code class="md-code">filter {
  if [type] == &quot;node_app&quot; {
    grok {
      patterns_dir =&gt; &quot;/opt/logstash/patterns&quot;
      match =&gt; { &quot;message&quot; =&gt; &quot;%{NODE_APP}&quot; }
    }
    date {
      match =&gt; [ &quot;timestamp&quot; , &quot;dd MMM HH:mm:ss&quot; ]
    }
  }
}
</code></pre> <p>Purrrfect! &#x1F42F;</p> <p>If you restart <code class="md-code md-code-inline">logstash</code>, &#x2013; either using the service or by executing the <code class="md-code md-code-inline">logstash -f logstash.conf</code> command again &#x2013; it should start streaming logs into Elasticsearch, from both <code class="md-code md-code-inline">nginx</code> and <code class="md-code md-code-inline">node</code>, as soon as the Logstash service comes online.</p> <p>How do we interact with those logs in a friendly manner then? While it&#x2019;s okay for search to be mostly accessible through the web interface of the blog, it&#x2019;d be pretty pointless to go through all of this trouble just to dump the logs into a table again on the web front-end.</p> <p>That&#x2019;s where <em>Kibana</em> comes in, <strong>helping us visualize the data.</strong></p> <h1 id="getting-started-with-kibana">Getting Started with Kibana</h1> <p>Having already installed the Elastic Stack, and after having set up Logstash to channel our logs into Elasticsearch, we&#x2019;re now ready to fire up Kibana and gain some insights into all our data.</p> <p>The default port Kibana listens on is <code class="md-code md-code-inline">5601</code>. I use <code class="md-code md-code-inline">nginx</code> as a reverse proxy in front of Kibana. You can use <code class="md-code md-code-inline">proxy_pass</code> to forward requests to the back-end Kibana <code class="md-code md-code-inline">node</code> app. Here&#x2019;s the full <code class="md-code md-code-inline">server</code> entry for <code class="md-code md-code-inline">nginx</code> I use for <a href="https://ponyfoo.com/" target="_blank" rel="noopener noreferrer" aria-label="That&apos;s my blog!">ponyfoo.com</a>.</p> <pre class="md-code-block"><code class="md-code">server {
  listen 8080 proxy_protocol;

  server_name analytics.ponyfoo.com;
  set_real_ip_from 172.31.0.0/20;
  real_ip_header proxy_protocol;
  access_log /var/log/nginx/analytics.ponyfoo.com.access.log metrics;

  auth_basic &quot;kibana_restricted&quot;;
  auth_basic_user_file /path/to/htpasswd/file;

  location / {
    proxy_pass http://127.0.0.1:5601;
    proxy_redirect off;
    proxy_http_version 1.1;
    proxy_set_header Host $http_host;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection $connection_upgrade;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_cache one;
    proxy_cache_key nx$request_uri$scheme;
  }
}
</code></pre> <p>Alternatively you can just skip <code class="md-code md-code-inline">nginx</code> altogether, that&#x2019;s up to you. I&#x2019;ve set it up in front of <code class="md-code md-code-inline">nginx</code> because I wanted basic authentication and I had already set up <code class="md-code md-code-inline">nginx</code> for the main blog application. Now you should be able to visit your Kibana dashboard at either <code class="md-code md-code-inline">http://localhost:5601</code> or your designated port and hostname.</p> <pre class="md-code-block"><code class="md-code md-lang-bash">open http://localhost:<span class="md-code-number">5601</span>
</code></pre> <p>The first time you load Kibana, you will be greeted with a message about configuring a default index pattern. Kibana suggests we try the <code class="md-code md-code-inline">logstash-*</code> index pattern. Logstash defaults to storing events on indices named according to the day the event is stored on, such as <code class="md-code md-code-inline">logstash-2016-06-17</code>, and so on. The <code class="md-code md-code-inline">*</code> in the pattern acts as a wildcard so that every index starting with <code class="md-code md-code-inline">logstash-</code> will match.</p> <figure><img alt="Set up a default index pattern." class="" src="https://i.imgur.com/HcJ1IW2.png"></figure> <p>Once we&#x2019;ve set up the index pattern, we might want to visit the <em>Discover</em> tab. Here, we&#x2019;ll be able to pull up Elasticsearch records as data rows, as well as a simple time-based chart displaying the amount of events registered over time. You can use the Elasticsearch query DSL to perform any queries and you can also apply filters on top of that query.</p> <figure><img alt="Events over time" class="" src="https://i.imgur.com/gvbDjts.png"></figure> <p>Data tables are nowhere near the most exciting kind of thing you can do in Kibana, though. Let&#x2019;s take a look at a few different charts I&#x2019;ve set up, and while we&#x2019;re at it I&#x2019;ll explain the terminology used by the Kibana interface as well as a few statistics terms in layman&#x2019;s language. Articles have been written before that describe in detail <a href="https://www.timroes.de/2015/02/07/kibana-4-tutorial-part-3-visualize/" target="_blank" rel="noopener noreferrer" aria-label="Kibana Tutorial &#x2013; Visualize">everything you can do with Kibana visualizations</a>, but I&#x2019;ll favor <em>a real-world example-based approach.</em></p> <p>Let&#x2019;s head to the <em>Visualize</em> tab for our first visualization.</p> <h1 id="charting-status-codes-and-request-volume">Charting Status Codes and Request Volume</h1> <p>Create a pie chart and select <code class="md-code md-code-inline">logstash-*</code> for the index pattern. You&#x2019;ll note that the default pie chart visualization is a whole pie containing every matching document and displaying the total count. Fair enough.</p> <figure><img alt="All requests in a pie" class="" src="https://i.imgur.com/H8e4QVy.png"></figure> <p>Add a <em>Split Slices</em> bucket with a <em>Terms</em> aggregation and choose the <code class="md-code md-code-inline">status</code> field <em>&#x2013; then hit Enter.</em> That sentence was overcharged with technical terms. Let&#x2019;s break it down. We are splitting the pie into several slices, using the Terms aggregation type to indicate that we want to identify distinct terms in the <code class="md-code md-code-inline">status</code> field, such as <code class="md-code md-code-inline">200</code>, <code class="md-code md-code-inline">302</code>, <code class="md-code md-code-inline">404</code>, and grouping documents by that aggregation. We&#x2019;ll end up with an slice for documents with a status code of <code class="md-code md-code-inline">200</code>, another slice for documents with a <code class="md-code md-code-inline">302</code> status code, and so on.</p> <p>Generally speaking, a healthy application should be returning mostly <code class="md-code md-code-inline">2xx</code> and <code class="md-code md-code-inline">3xx</code> responses. This chart makes it easy to visualize whether that&#x2019;s the case.</p> <figure><img alt="Splitting slices gives us an slice per status code" class="" src="https://i.imgur.com/akJ57DG.png"></figure> <p>You can further specialize the graph by adding a sub-bucket, aggregating again using Terms, this time by the <code class="md-code md-code-inline">request.raw</code> field. That way, you&#x2019;ll not only get a general ratio of status codes, but you&#x2019;ll also gain insight into which requests are the most popular, as well as which ones are the most problematic, all in one chart.</p> <p>In the screenshot found below, you can see that the <code class="md-code md-code-inline">/articles/feed</code> route is getting even more traffic than the home page.</p> <figure><img alt="Splitting slices all the way down" class="" src="https://i.imgur.com/v59RwhY.png"></figure> <p>Maybe a little bit of caching is in order, since the RSS feeds don&#x2019;t change all that often. In the same light, earlier on these pie charts helped me identify and fix a bunch of requests for icons and images that were yielding <code class="md-code md-code-inline">404</code> errors. That&#x2019;s something that would&#x2019;ve been harder to spot by looking at the dense and raw nginx logs, but it&#x2019;s really easy to take action based on visualizations that help us work with the data.</p> <p>Save the pie chart using the <em>Save</em> link on the top bar and give it a name such as &#x201C;Response Stats&#x201D;.</p> <h1 id="plotting-responses-over-time-by-status-code">Plotting Responses Over Time, By Status Code</h1> <p>Go back to the Visualize tab and create a new line chart. You&#x2019;ll see that we only get a single dot graphed, because there&#x2019;s no <code class="md-code md-code-inline">x</code> axis by default. Let&#x2019;s add an <em>X-Axis</em>, using a <em>Date Histogram</em> of <code class="md-code md-code-inline">@timestamp</code>. This is a fancy way of saying we want to group our data points into 30 minute long buckets. All events within a 30-minute window are grouped <em>&#x2013; aggregated &#x2013;</em> into a single bucket displaying the Count <em>(that&#x2019;s the <code class="md-code md-code-inline">y</code> axis)</em>. A <em>Date Histogram</em> makes it possible to render a large dataset into a line chart without having to render several thousands of individual data points. The Date Histogram aggregation allows you to define the time lapse you want to use to group events, or you can use the default <em>Auto</em> value of 30 minutes, as with the example below.</p> <figure><img alt="Responses plotted over time" class="" src="https://i.imgur.com/yGwfCmd.png"></figure> <p>While it&#x2019;s useful to render requests over time, particularly when it comes to determining whether our server is functioning properly under load, experiencing a sudden surge of requests, or not responding to a single request, it&#x2019;s even more useful to understand the underlying sub-segments of data. What are the status codes for each of those responses over time? Was the spike that we saw in the earlier chart the product of <code class="md-code md-code-inline">500</code> errors or just a spike in traffic?</p> <p>Add a <em>Split Lines</em> aggregation by Terms using the <code class="md-code md-code-inline">status</code> field. After hitting Enter <em>(or the Play button)</em>, we&#x2019;ll note that the chart is now divided into several different lines, each line representing all responses that ended with a given status code. Now it&#x2019;ll be much easier to spot error spikes and discern them from spikes in traffic.</p> <figure><img alt="Responses plotted over time splitted by status code" class="" src="https://i.imgur.com/z71sMH5.png"></figure> <p>Save this chart as <em>&#x201C;Response Times&#x201D;</em>, and let&#x2019;s create something else.</p> <h1 id="charting-geographic-data">Charting Geographic Data</h1> <p>It&#x2019;s really easy to chart geographic data into a map using Kibana, provided that you&#x2019;ve set up the <code class="md-code md-code-inline">geoip</code> filter in Logstash as indicated earlier, so that the IP addresses found in nginx logs are transformed into geolocation data that can be charted into a map. Given that, it&#x2019;s just a matter of going to the Kibana Visualize tab, creating a Tile Map, and adding a Geo Coordinates bucket using the field computed by Logstash.</p> <figure><img alt="Geolocation map using Kibana and Logstash" class="" src="https://i.imgur.com/Um1NzC4.png"></figure> <p>Save the map as <em>&#x201C;Request Locations&#x201D;</em>.</p> <h1 id="stitching-it-all-together">Stitching It All Together</h1> <p>Now that we&#x2019;ve created three different visualizations, where we have a rough overview of status codes and the hottest endpoints, time-based plots of each of the status codes, and a map showing where requests are coming from, it&#x2019;d be nice to put all of that in a single dashboard.</p> <p>Visit the <em>Dashboard</em> tab in Kibana, pick <em>Add</em> from the top-bar navigation and add all of our saved charts. After some rearranging of the charts, you&#x2019;ll get a dashboard like the one below. You can save this one as well, naming it something like <em>&#x201C;Overview&#x201D;</em>. Whenever you want to get a glimpse into your operations now you can simply visit the Overview dashboard and you&#x2019;ll be able to collect insights from it.</p> <figure><img alt="Our first dashboard" class="" src="https://i.imgur.com/coOul7v.png"></figure> <p>I&#x2019;ve also included an &#x201C;average response time&#x201D; over time chart in my dashboard, which wasn&#x2019;t discussed earlier. You can plot that on a line chart with a <code class="md-code md-code-inline">y</code> axis of a <em>Date Histogram</em> with minute or second resolution, and an <code class="md-code md-code-inline">x</code> axis using an <em>Average</em> aggregation of <code class="md-code md-code-inline">request_time</code>.</p> <blockquote> <p>Make sure to save it afterwards!</p> </blockquote></div>
